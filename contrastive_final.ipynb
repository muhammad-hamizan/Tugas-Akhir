{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasifikasi Surah Al-Qur'an - Contrastive\n",
    "\n",
    "### Fine-Tuning Wav2Vec\n",
    "\n",
    "Notebook ini melakukan fine-tuning pada base Wav2Vec.\n",
    "1. Wav2Vec base dilatih dengan LR = 5e-6.\n",
    "2. Projection head dilatih dengan LR = 1e-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Import Pustaka & Pengaturan Awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import audiomentations as A\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Memuat Dataset (Dengan Reciter-Wise Split & Metadata Penuh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"audio_data_processed\"\n",
    "\n",
    "all_qaris = [d for d in sorted(os.listdir(DATASET_ROOT)) if os.path.isdir(os.path.join(DATASET_ROOT, d))]\n",
    "print(f\"Found {len(all_qaris)} unique reciters.\")\n",
    "train_qaris, test_val_qaris = train_test_split(all_qaris, test_size=0.30, random_state=42)\n",
    "test_qaris, _ = train_test_split(test_val_qaris, test_size=0.50, random_state=42)\n",
    "print(f\"Training reciters ({len(train_qaris)}): {train_qaris}\")\n",
    "print(f\"Test reciters ({len(test_qaris)}): {test_qaris}\")\n",
    "\n",
    "def get_files_and_metadata(qari_list):\n",
    "    file_paths = []\n",
    "    metadata = []\n",
    "    for qari_folder in qari_list:\n",
    "        qari_path = os.path.join(DATASET_ROOT, qari_folder)\n",
    "        for filename in os.listdir(qari_path):\n",
    "            if filename.endswith(\".mp3\"):\n",
    "                file_id = filename.split('.')[0]\n",
    "                if len(file_id) != 6 or not file_id.isdigit():\n",
    "                    continue\n",
    "                surah_label = file_id[:3]\n",
    "                ayah_label = file_id[3:]\n",
    "                full_path = os.path.join(DATASET_ROOT, qari_folder, filename)\n",
    "                file_paths.append(full_path)\n",
    "                metadata.append({\n",
    "                    \"reciter\": qari_folder,\n",
    "                    \"surah\": surah_label,\n",
    "                    \"ayah\": ayah_label,\n",
    "                    \"full_ayah_id\": f\"{surah_label}-{ayah_label}\"\n",
    "                })\n",
    "    return file_paths, metadata\n",
    "\n",
    "X_train_paths, train_metadata = get_files_and_metadata(train_qaris)\n",
    "X_test_paths, test_metadata = get_files_and_metadata(test_qaris)\n",
    "y_train_surah_labels = [m['surah'] for m in train_metadata]\n",
    "y_test_surah_labels = [m['surah'] for m in test_metadata]\n",
    "all_labels_for_encoder = y_train_surah_labels + y_test_surah_labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels_for_encoder)\n",
    "class_names = label_encoder.classes_\n",
    "NUM_CLASSES = len(class_names)\n",
    "y_train = label_encoder.transform(y_train_surah_labels)\n",
    "y_test = label_encoder.transform(y_test_surah_labels)\n",
    "for i, meta in enumerate(train_metadata):\n",
    "    meta['encoded_surah_label'] = y_train[i]\n",
    "print(f\"\\nUkuran data latih: {len(X_train_paths)} file\")\n",
    "print(f\"Ukuran data uji: {len(X_test_paths)} file\")\n",
    "print(f\"Jumlah kelas (surah): {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Preprocessing & Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5 \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, paths, labels, target_sr=SAMPLE_RATE, duration_s=DURATION, is_train=False):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.target_sr = target_sr\n",
    "        self.num_samples = self.target_sr * duration_s\n",
    "        self.is_train = is_train\n",
    "        if self.is_train:\n",
    "            self.augment = A.Compose([\n",
    "                A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "                A.TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "                A.PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "            ])\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saat memuat file {path}: {e}\")\n",
    "            return torch.zeros(self.num_samples), -1\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        if sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        if self.is_train:\n",
    "            samples_np = waveform.numpy().squeeze()\n",
    "            augmented_samples = self.augment(samples=samples_np, sample_rate=self.target_sr)\n",
    "            waveform = torch.from_numpy(augmented_samples).unsqueeze(0)\n",
    "        if waveform.shape[1] > self.num_samples:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "        else:\n",
    "            pad_size = self.num_samples - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n",
    "        return waveform.squeeze(0), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Model & Arsitektur (Contrastive - Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Model akan berjalan di: {device.type}\")\n",
    "\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.wav2vec = base_model\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.wav2vec(x).last_hidden_state\n",
    "        \n",
    "        pooled_output = torch.mean(outputs, dim=1)\n",
    "        embedding = self.projection(pooled_output)\n",
    "        return embedding\n",
    "\n",
    "contrastive_net = ContrastiveModel(wav2vec_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Implementasi & Pelatihan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Definisi Triplet Dataset (Logika Hard Mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, paths, metadata, dataset_class, is_train=False):\n",
    "        self.paths = paths\n",
    "        self.metadata = metadata\n",
    "        self.encoded_surah_labels = np.array([m['encoded_surah_label'] for m in metadata])\n",
    "        self.dataset = dataset_class(self.paths, self.encoded_surah_labels, is_train=is_train)\n",
    "        self.ayah_to_indices = defaultdict(list)\n",
    "        self.reciter_surah_to_indices = defaultdict(lambda: defaultdict(list))\n",
    "        self.surah_to_indices = defaultdict(list)\n",
    "        for i, meta in enumerate(self.metadata):\n",
    "            self.ayah_to_indices[meta['full_ayah_id']].append(i)\n",
    "            self.reciter_surah_to_indices[meta['reciter']][meta['surah']].append(i)\n",
    "            self.surah_to_indices[meta['surah']].append(i)\n",
    "        self.all_surahs = list(self.surah_to_indices.keys())\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def _find_positive(self, anchor_meta, anchor_index):\n",
    "        anchor_ayah_id = anchor_meta['full_ayah_id']\n",
    "        anchor_reciter = anchor_meta['reciter']\n",
    "        candidate_indices = self.ayah_to_indices.get(anchor_ayah_id, [])\n",
    "        different_reciters = [idx for idx in candidate_indices if self.metadata[idx]['reciter'] != anchor_reciter]\n",
    "        if different_reciters:\n",
    "            return np.random.choice(different_reciters)\n",
    "        same_reciter_diff_file = [idx for idx in candidate_indices if idx != anchor_index]\n",
    "        if same_reciter_diff_file:\n",
    "            return np.random.choice(same_reciter_diff_file)\n",
    "        return anchor_index\n",
    "    def _find_negative(self, anchor_meta, anchor_index):\n",
    "        anchor_surah = anchor_meta['surah']\n",
    "        anchor_reciter = anchor_meta['reciter']\n",
    "        reciter_surahs = self.reciter_surah_to_indices.get(anchor_reciter, {})\n",
    "        different_surahs = [surah for surah in reciter_surahs.keys() if surah != anchor_surah]\n",
    "        if different_surahs:\n",
    "            neg_surah = np.random.choice(different_surahs)\n",
    "            neg_index = np.random.choice(reciter_surahs[neg_surah])\n",
    "            return neg_index\n",
    "        neg_surah = anchor_surah\n",
    "        while neg_surah == anchor_surah:\n",
    "            neg_surah = np.random.choice(self.all_surahs)\n",
    "        neg_index = np.random.choice(self.surah_to_indices[neg_surah])\n",
    "        return neg_index\n",
    "    def __getitem__(self, index):\n",
    "        anchor_meta = self.metadata[index]\n",
    "        anchor_waveform, _ = self.dataset[index]\n",
    "        positive_index = self._find_positive(anchor_meta, index)\n",
    "        positive_waveform, _ = self.dataset[positive_index]\n",
    "        negative_index = self._find_negative(anchor_meta, index)\n",
    "        negative_waveform, _ = self.dataset[negative_index]\n",
    "        return anchor_waveform, positive_waveform, negative_waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Menjalankan Eksperimen (Contrastive Loss - Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 24\n",
    "NUM_WORKERS = 8\n",
    "PROJECTION_LR = 1e-4\n",
    "WAV2VEC_LR = 5e-6\n",
    "NUM_EPOCHS = 100\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Training Setup ---\n",
    "triplet_train_dataset = TripletDataset(X_train_paths, train_metadata, AudioDataset, is_train=True)\n",
    "train_loader = DataLoader(triplet_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"Training DataLoader is ready.\")\n",
    "\n",
    "# --- Model & Optimizer --- #\n",
    "optimizer = optim.Adam([\n",
    "    {'params': contrastive_net.wav2vec.parameters(), 'lr': WAV2VEC_LR},\n",
    "    {'params': contrastive_net.projection.parameters(), 'lr': PROJECTION_LR}\n",
    "])\n",
    "\n",
    "# --- Loss & Scaler ---\n",
    "contrastive_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"\\nStarting Training (Contrastive Loss - Fine-Tuned)...\")\n",
    "\n",
    "# --- Early Stopping Variables ---\n",
    "patience = 10\n",
    "epochs_no_improve = 0\n",
    "best_train_loss = float('inf')\n",
    "model_save_path = \"contrastive_model.pth\"\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    contrastive_net.train()\n",
    "    total_train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: N/A\")\n",
    "    \n",
    "    for anchor, positive, negative in progress_bar:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            embed_a = contrastive_net(anchor)\n",
    "            embed_p = contrastive_net(positive)\n",
    "            embed_n = contrastive_net(negative)\n",
    "            loss = contrastive_loss_fn(embed_a, embed_p, embed_n)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"   -> Epoch {epoch+1} Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # --- Early Stopping & Saving Logic ---\n",
    "    if avg_train_loss < best_train_loss:\n",
    "        best_train_loss = avg_train_loss\n",
    "        torch.save(contrastive_net.state_dict(), model_save_path)\n",
    "        print(f\"   -> New best model saved to {model_save_path} with loss: {best_train_loss:.4f}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"   -> No improvement for {epochs_no_improve} epoch(s). Best loss remains {best_train_loss:.4f}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nPelatihan contrastive selesai.\")\n",
    "print(f\"Model terbaik disimpan di: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Evaluasi pada Data Uji (Test Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Memuat Bobot Model Terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_MODEL_PATH = \"contrastive_model.pth\"\n",
    "\n",
    "evaluation_model = ContrastiveModel(wav2vec_model).to(device)\n",
    "evaluation_model.load_state_dict(torch.load(EVALUATION_MODEL_PATH))\n",
    "\n",
    "print(f\"Model weights from {EVALUATION_MODEL_PATH} loaded successfully for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Ekstraksi Embedding & Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, paths, labels, dataset_class):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    dataset = dataset_class(paths, labels, is_train=False)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, _ in tqdm(loader, desc=\"Mengekstrak Embedding\"):\n",
    "            waveforms = waveforms.to(device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                 embeds = model(waveforms)\n",
    "                 embeddings.append(embeds.cpu().numpy())\n",
    "            \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"Mengekstrak embedding final dari data latih dan uji...\")\n",
    "X_train_embed = extract_embeddings(evaluation_model, X_train_paths, y_train, AudioDataset)\n",
    "X_test_embed = extract_embeddings(evaluation_model, X_test_paths, y_test, AudioDataset)\n",
    "print(\"Ekstraksi embedding selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Klasifikasi dengan k-NN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melatih Classifier k-NN (Baseline)...\")\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "knn_classifier.fit(X_train_embed, y_train)\n",
    "y_pred_knn = knn_classifier.predict(X_test_embed)\n",
    "print(\"Pelatihan k-NN selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Klasifikasi dengan MLP (Pembanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melatih Classifier MLP (Pembanding)...\")\n",
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128), \n",
    "    max_iter=500, \n",
    "    random_state=42, \n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "mlp_classifier.fit(X_train_embed, y_train)\n",
    "y_pred_mlp = mlp_classifier.predict(X_test_embed)\n",
    "print(\"Pelatihan MLP selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 7. Hasil Evaluasi Kinerja Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(f\"HASIL EVALUASI MODEL DARI: {EVALUATION_MODEL_PATH}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n--- Kinerja k-Nearest Neighbors (k-NN) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(\"\\nLaporan Klasifikasi:\")\n",
    "print(classification_report(y_test, y_pred_knn, target_names=class_names, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Kinerja Multilayer Perceptron (MLP) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(\"\\nLaporan Klasifikasi:\")\n",
    "print(classification_report(y_test, y_pred_mlp, target_names=class_names, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
