{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Demo: Full Replication (k-NN & MLP)\n",
    "\n",
    "**Author:** Muhammad Rafie Hamizan\n",
    "\n",
    "This notebook replicates the full evaluation methodology using both **k-NN** and **MLP** classifiers.\n",
    "\n",
    "### Methodology\n",
    "1. **Feature Extraction**: Extracts embeddings from the **Full Training Set** (Reference) and **Full Test Set** (Query) using the fine-tuned Wav2Vec 2.0 model.\n",
    "2. **k-NN Evaluation**: Classifies test data based on geometric distance to training data.\n",
    "3. **MLP Evaluation**: Trains a neural network classifier on the extracted training embeddings and predicts the test set.\n",
    "4. **Comparative Visualization**: Visualizes the decision boundaries of both models using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATASET_ROOT = \"audio_data_processed\"\n",
    "MODEL_PATH = \"contrastive_model.pth\"\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Pipeline (Reciter-Wise Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Reciter Splits ---\n",
    "all_qaris = [d for d in sorted(os.listdir(DATASET_ROOT)) if os.path.isdir(os.path.join(DATASET_ROOT, d))]\n",
    "train_qaris, test_val_qaris = train_test_split(all_qaris, test_size=0.30, random_state=42)\n",
    "test_qaris, _ = train_test_split(test_val_qaris, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Training Reciters ({len(train_qaris)}): {train_qaris}\")\n",
    "print(f\"Test Reciters ({len(test_qaris)}): {test_qaris}\")\n",
    "\n",
    "# --- 2. Helper to get files ---\n",
    "def get_files_and_metadata(qari_list):\n",
    "    file_paths = []\n",
    "    surah_labels = []\n",
    "    \n",
    "    for qari_folder in qari_list:\n",
    "        qari_path = os.path.join(DATASET_ROOT, qari_folder)\n",
    "        if not os.path.exists(qari_path): continue\n",
    "        for filename in os.listdir(qari_path):\n",
    "            if filename.endswith(\".mp3\"):\n",
    "                file_id = filename.split('.')[0]\n",
    "                if len(file_id) != 6 or not file_id.isdigit(): continue\n",
    "                surah_label = file_id[:3]\n",
    "                file_paths.append(os.path.join(DATASET_ROOT, qari_folder, filename))\n",
    "                surah_labels.append(surah_label)\n",
    "    return file_paths, surah_labels\n",
    "\n",
    "# --- 3. Load File Paths ---\n",
    "print(\"\\nScanning files...\")\n",
    "X_train_paths, y_train_raw = get_files_and_metadata(train_qaris)\n",
    "X_test_paths, y_test_raw = get_files_and_metadata(test_qaris)\n",
    "\n",
    "print(f\"Training Files: {len(X_train_paths)}\")\n",
    "print(f\"Test Files:     {len(X_test_paths)}\")\n",
    "\n",
    "# --- 4. Encode Labels ---\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_raw + y_test_raw)\n",
    "y_train_encoded = label_encoder.transform(y_train_raw)\n",
    "y_test_encoded = label_encoder.transform(y_test_raw)\n",
    "class_names = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, paths, labels, target_sr=16000, duration_s=5):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.target_sr = target_sr\n",
    "        self.num_samples = target_sr * duration_s\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "        except: return torch.zeros(1, self.num_samples), -1\n",
    "        if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        if sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        if waveform.shape[1] > self.num_samples:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "        else:\n",
    "            pad_size = self.num_samples - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n",
    "        return waveform.squeeze(0), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Loading & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.wav2vec = base_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256), nn.ReLU(), nn.Linear(256, 128)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        outputs = self.wav2vec(x).last_hidden_state\n",
    "        pooled_output = torch.mean(outputs, dim=1)\n",
    "        embedding = self.projection(pooled_output)\n",
    "        return embedding\n",
    "\n",
    "wav2vec_base = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = ContrastiveModel(wav2vec_base).to(DEVICE)\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Missing {MODEL_PATH}\")\n",
    "\n",
    "def extract_embeddings(paths, labels, desc):\n",
    "    dataset = AudioDataset(paths, labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
    "    embeddings, labels_out = [], []\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labs in tqdm(loader, desc=desc):\n",
    "            waveforms = waveforms.to(DEVICE)\n",
    "            embeds = model(waveforms)\n",
    "            embeddings.append(embeds.cpu().numpy())\n",
    "            labels_out.append(labs.numpy())\n",
    "    return np.vstack(embeddings), np.concatenate(labels_out)\n",
    "\n",
    "print(\"Extracting Training Data...\")\n",
    "X_train_embed, y_train_labels = extract_embeddings(X_train_paths, y_train_encoded, desc=\"Train\")\n",
    "print(\"Extracting Test Data...\")\n",
    "X_test_embed, y_test_labels = extract_embeddings(X_test_paths, y_test_encoded, desc=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifier 1: k-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training k-NN...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "knn.fit(X_train_embed, y_train_labels)\n",
    "y_pred_knn = knn.predict(X_test_embed)\n",
    "\n",
    "acc_knn = accuracy_score(y_test_labels, y_pred_knn)\n",
    "print(f\"\\n>>> k-NN Accuracy: {acc_knn:.4f}\")\n",
    "print(classification_report(y_test_labels, y_pred_knn, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classifier 2: Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MLP (on extracted embeddings)...\")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train_embed, y_train_labels)\n",
    "y_pred_mlp = mlp.predict(X_test_embed)\n",
    "\n",
    "acc_mlp = accuracy_score(y_test_labels, y_pred_mlp)\n",
    "print(f\"\\n>>> MLP Accuracy: {acc_mlp:.4f}\")\n",
    "print(classification_report(y_test_labels, y_pred_mlp, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparative Visualization (t-SNE)\n",
    "We visualize the Test Set embeddings colored by: **Ground Truth**, **k-NN Prediction**, and **MLP Prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running t-SNE projection on Test Data...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_test_2d = tsne.fit_transform(X_test_embed)\n",
    "\n",
    "# Setup Plotting Data\n",
    "df_viz = pd.DataFrame({\n",
    "    'x': X_test_2d[:, 0],\n",
    "    'y': X_test_2d[:, 1],\n",
    "    'Ground Truth': [class_names[i] for i in y_test_labels],\n",
    "    'k-NN Pred': [class_names[i] for i in y_pred_knn],\n",
    "    'MLP Pred': [class_names[i] for i in y_pred_mlp]\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=True)\n",
    "\n",
    "# 1. Ground Truth\n",
    "sns.scatterplot(ax=axes[0], data=df_viz, x='x', y='y', hue='Ground Truth', palette='husl', s=50, alpha=0.7, legend=False)\n",
    "axes[0].set_title(f\"Ground Truth Labels\\n(Target)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. k-NN Predictions\n",
    "sns.scatterplot(ax=axes[1], data=df_viz, x='x', y='y', hue='k-NN Pred', palette='husl', s=50, alpha=0.7, legend=False)\n",
    "axes[1].set_title(f\"k-NN Predictions\\n(Accuracy: {acc_knn:.4f})\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. MLP Predictions\n",
    "sns.scatterplot(ax=axes[2], data=df_viz, x='x', y='y', hue='MLP Pred', palette='husl', s=50, alpha=0.7, legend=False)\n",
    "axes[2].set_title(f\"MLP Predictions\\n(Accuracy: {acc_mlp:.4f})\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_tsne_knn_mlp.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'comparison_tsne_knn_mlp.png'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
