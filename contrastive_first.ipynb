{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasifikasi Surah Al-Qur'an - Versi `torchaudio` / FFmpeg\n",
    "\n",
    "### Versi Lanjutan - Dengan Deeper Fine-Tuning & Reciter-Wise Split\n",
    "\n",
    "Notebook ini menggunakan `torchaudio` untuk memuat audio, yang memerlukan instalasi FFmpeg yang dapat diakses oleh Python.\n",
    "\n",
    "**Fitur Kunci**:\n",
    "- **Audio Backend**: Menggunakan `torchaudio`.\n",
    "- **Deeper Fine-Tuning**: Melatih dua layer teratas dari Wav2Vec 2.0.\n",
    "- **Reciter-Wise Split**: Menggunakan metode splitting terbaik untuk evaluasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Import Pustaka & Pengaturan Awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Memuat Dataset (Dengan Reciter-Wise Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = r\"audio_data_processed\"\n",
    "\n",
    "all_qaris = [d for d in sorted(os.listdir(DATASET_ROOT)) if os.path.isdir(os.path.join(DATASET_ROOT, d))]\n",
    "print(f\"Found {len(all_qaris)} unique reciters.\")\n",
    "\n",
    "train_qaris, test_qaris = train_test_split(all_qaris, test_size=0.15, random_state=42)\n",
    "train_qaris, val_qaris = train_test_split(train_qaris, test_size=(15/85), random_state=42)\n",
    "\n",
    "print(f\"Training reciters ({len(train_qaris)}): {train_qaris}\")\n",
    "print(f\"Validation reciters ({len(val_qaris)}): {val_qaris}\")\n",
    "print(f\"Test reciters ({len(test_qaris)}): {test_qaris}\")\n",
    "\n",
    "def get_files_for_qaris(qari_list):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for qari_folder in qari_list:\n",
    "        qari_path = os.path.join(DATASET_ROOT, qari_folder)\n",
    "        for filename in os.listdir(qari_path):\n",
    "            if filename.endswith(\".mp3\") and len(filename) >= 6:\n",
    "                surah_label = filename[:3]\n",
    "                full_path = os.path.join(qari_path, filename)\n",
    "                file_paths.append(full_path)\n",
    "                labels.append(surah_label)\n",
    "    return file_paths, labels\n",
    "\n",
    "X_train_paths, y_train_labels = get_files_for_qaris(train_qaris)\n",
    "X_val_paths, y_val_labels = get_files_for_qaris(val_qaris)\n",
    "X_test_paths, y_test_labels = get_files_for_qaris(test_qaris)\n",
    "\n",
    "all_labels = y_train_labels + y_val_labels + y_test_labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "y_train = label_encoder.transform(y_train_labels)\n",
    "y_val = label_encoder.transform(y_val_labels)\n",
    "y_test = label_encoder.transform(y_test_labels)\n",
    "\n",
    "print(f\"\\nUkuran data latih: {len(X_train_paths)} file\")\n",
    "print(f\"Ukuran data validasi: {len(X_val_paths)} file\")\n",
    "print(f\"Ukuran data uji: {len(X_test_paths)} file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 3. Preprocessing & Dataset Loader (with `torchaudio`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5 \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, paths, labels, target_sr=SAMPLE_RATE, duration_s=DURATION):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.target_sr = target_sr\n",
    "        self.num_samples = self.target_sr * duration_s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saat memuat file {path}: {e}\")\n",
    "            return torch.zeros(self.num_samples), label\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        if sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        if waveform.shape[1] > self.num_samples:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "        else:\n",
    "            pad_size = self.num_samples - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n",
    "            \n",
    "        return waveform.squeeze(0), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 4. Model & Arsitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Model akan berjalan di: {device.type}\")\n",
    "\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.wav2vec = base_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.wav2vec(x).last_hidden_state\n",
    "        pooled_output = torch.mean(outputs, dim=1)\n",
    "        embedding = self.projection(pooled_output)\n",
    "        return embedding\n",
    "\n",
    "contrastive_net = ContrastiveModel(wav2vec_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Pembelajaran Representasi dengan Deeper Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, paths, labels, dataset_class):\n",
    "        self.paths = paths\n",
    "        self.labels = np.array(labels)\n",
    "        self.dataset = dataset_class(self.paths, self.labels)\n",
    "        self.labels_set = set(self.labels)\n",
    "        self.label_to_indices = {label: np.where(self.labels == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path = self.paths[index]\n",
    "        anchor_label = self.labels[index]\n",
    "        anchor_waveform, _ = self.dataset[index]\n",
    "\n",
    "        positive_index = index\n",
    "        if len(self.label_to_indices[anchor_label]) > 1:\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(self.label_to_indices[anchor_label])\n",
    "        \n",
    "        positive_waveform, _ = self.dataset[positive_index]\n",
    "\n",
    "        negative_label = np.random.choice(list(self.labels_set - {anchor_label}))\n",
    "        negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "        negative_waveform, _ = self.dataset[negative_index]\n",
    "        \n",
    "        return anchor_waveform, positive_waveform, negative_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "PROJECTION_HEAD_LR = 1e-4\n",
    "WAV2VEC_LR = 5e-6 \n",
    "NUM_EPOCHS_CONTRASTIVE = 100\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"NUM_WORKERS: {NUM_WORKERS}\")\n",
    "print(f\"PROJECTION_HEAD_LR: {PROJECTION_HEAD_LR}\")\n",
    "print(f\"WAV2VEC_LR: {WAV2VEC_LR}\")\n",
    "print(f\"NUM_EPOCHS: {NUM_EPOCHS_CONTRASTIVE}\")\n",
    "\n",
    "# --- Training Setup ---\n",
    "triplet_train_dataset = TripletDataset(X_train_paths, y_train, AudioDataset)\n",
    "train_loader = DataLoader(triplet_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Unfreeze DUA layer teratas dari Wav2Vec 2.0\n",
    "for param in contrastive_net.wav2vec.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in contrastive_net.wav2vec.encoder.layers[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "print(\"\\nDua layer teratas Wav2Vec 2.0 telah di-unfreeze untuk fine-tuning.\")\n",
    "\n",
    "# Buat parameter groups untuk learning rate yang berbeda\n",
    "optimizer = optim.Adam([\n",
    "    {'params': contrastive_net.projection.parameters(), 'lr': PROJECTION_HEAD_LR},\n",
    "    {'params': contrastive_net.wav2vec.encoder.layers[-2:].parameters(), 'lr': WAV2VEC_LR}\n",
    "])\n",
    "\n",
    "loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"\\nStarting Deeper Fine-Tuning...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_CONTRASTIVE):\n",
    "    contrastive_net.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS_CONTRASTIVE} | Loss: N/A\")\n",
    "    \n",
    "    for anchor, positive, negative in progress_bar:\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            anchor_embed = contrastive_net(anchor)\n",
    "            positive_embed = contrastive_net(positive)\n",
    "            negative_embed = contrastive_net(negative)\n",
    "            loss = loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{NUM_EPOCHS_CONTRASTIVE} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"    -> Epoch {epoch+1} Selesai, Rata-rata Contrastive Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Ekstraksi Embedding & Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, paths, labels, dataset_class):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    dataset = dataset_class(paths, labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, _ in tqdm(loader, desc=\"Mengekstrak Embedding\"):\n",
    "            waveforms = waveforms.to(device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                embeds = model(waveforms).cpu().numpy()\n",
    "            embeddings.append(embeds)\n",
    "            \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "print(\"Mengekstrak embedding final dari data latih dan uji...\")\n",
    "X_train_embed = extract_embeddings(contrastive_net, X_train_paths, y_train, AudioDataset)\n",
    "X_test_embed = extract_embeddings(contrastive_net, X_test_paths, y_test, AudioDataset)\n",
    "print(\"Ekstraksi embedding selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Klasifikasi dengan k-NN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melatih Classifier k-NN (Baseline)...\")\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "knn_classifier.fit(X_train_embed, y_train)\n",
    "y_pred_knn = knn_classifier.predict(X_test_embed)\n",
    "print(\"Pelatihan k-NN selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Klasifikasi dengan MLP (Pembanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melatih Classifier MLP (Pembanding)...\")\n",
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128), \n",
    "    max_iter=500, \n",
    "    random_state=42, \n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "mlp_classifier.fit(X_train_embed, y_train)\n",
    "y_pred_mlp = mlp_classifier.predict(X_test_embed)\n",
    "print(\"Pelatihan MLP selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 7. Evaluasi Kinerja Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"HASIL EVALUASI MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Kinerja k-Nearest Neighbors (k-NN) ---\n",
    "print(\"\\n--- Kinerja k-Nearest Neighbors (k-NN) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(\"\\nLaporan Klasifikasi:\")\n",
    "knn_report_labels = np.unique(np.concatenate((y_test, y_pred_knn)))\n",
    "knn_report_names = label_encoder.inverse_transform(knn_report_labels)\n",
    "print(classification_report(y_test, y_pred_knn, labels=knn_report_labels, target_names=knn_report_names, zero_division=0))\n",
    "\n",
    "# --- Kinerja Multilayer Perceptron (MLP) ---\n",
    "print(\"\\n--- Kinerja Multilayer Perceptron (MLP) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(\"\\nLaporan Klasifikasi:\")\n",
    "mlp_report_labels = np.unique(np.concatenate((y_test, y_pred_mlp)))\n",
    "mlp_report_names = label_encoder.inverse_transform(mlp_report_labels)\n",
    "print(classification_report(y_test, y_pred_mlp, labels=mlp_report_labels, target_names=mlp_report_names, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
